{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "201bbdb3-0419-452d-8ff1-15affde9797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "from typing import List, Optional\n",
    "from torch import Tensor\n",
    "import copy\n",
    "import functools\n",
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92fe28b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d29ed65-5812-42fb-9c2f-433f0d058032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This is used as a decorator (I am not quite sure what this is) on the step() method in the custom optimizer\n",
    "#### I copy pasted it from PyTorch as it is needed to compute and track the gradients in the background, which is the basic functionality of PyTorch\n",
    "def _use_grad_for_differentiable(func):\n",
    "    def _use_grad(self, *args, **kwargs):\n",
    "        import torch._dynamo\n",
    "        prev_grad = torch.is_grad_enabled()\n",
    "        try:\n",
    "            # Note on graph break below:\n",
    "            # we need to graph break to ensure that aot respects the no_grad annotation.\n",
    "            # This is important for perf because without this, functionalization will generate an epilogue\n",
    "            # which updates the mutated parameters of the optimizer which is *not* visible to inductor, as a result,\n",
    "            # inductor will allocate for every parameter in the model, which is horrible.\n",
    "            # With this, aot correctly sees that this is an inference graph, and functionalization will generate\n",
    "            # an epilogue which is appended to the graph, which *is* visible to inductor, as a result, inductor sees that\n",
    "            # step is in place and is able to avoid the extra allocation.\n",
    "            # In the future, we will either 1) continue to graph break on backward, so this graph break does not matter\n",
    "            # or 2) have a fully fused forward and backward graph, which will have no_grad by default, and we can remove this\n",
    "            # graph break to allow the fully fused fwd-bwd-optimizer graph to be compiled.\n",
    "            # see https://github.com/pytorch/pytorch/issues/104053\n",
    "            torch.set_grad_enabled(self.defaults['differentiable'])\n",
    "            torch._dynamo.graph_break()\n",
    "            ret = func(self, *args, **kwargs)\n",
    "        finally:\n",
    "            torch._dynamo.graph_break()\n",
    "            torch.set_grad_enabled(prev_grad)\n",
    "        return ret\n",
    "    functools.update_wrapper(_use_grad, func)\n",
    "    return _use_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f53de37-0821-4e90-887b-bf31b0ed4bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define the custom optimizer\n",
    "class SRPROP(Optimizer):\n",
    "    def __init__(self, params, M=1, L=1, lr=1e-2, etas=(0.5, 1.2), step_sizes=(1e-6, 50),\n",
    "                 *, differentiable: bool = False, ):\n",
    "            if not 0.0 <= lr:\n",
    "                raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "            if not 0.0 < etas[0] < 1.0 < etas[1]:\n",
    "                raise ValueError(f\"Invalid eta values: {etas[0]}, {etas[1]}\")\n",
    "            if not M!=L:\n",
    "                raise ValueError(f\"For M=L, use Rprop\")\n",
    "            if not L%M==0:\n",
    "                raise ValueError(f\"L={L} must be integer multiple of M={M}\")\n",
    "        #### Make hyperparameters accessible through a dictionary\n",
    "            defaults = dict(\n",
    "            M=M,\n",
    "            L=L,\n",
    "            lr=lr,\n",
    "            etas=etas,\n",
    "            step_sizes=step_sizes,\n",
    "            differentiable=differentiable,)\n",
    "        #### super() makes the class inherit properties from PyTorch's Optimizer class\n",
    "            super(SRPROP, self).__init__(params, defaults)\n",
    "        #### Giving the class attributes that can be accessed later to update learning rates\n",
    "            self.step_sizes = []  # Initialize step_sizes attribute\n",
    "            self.lr_counter = 0  # Counts learning rate updates\n",
    "            self.data_tally = 0  # Counts volume of data fed through\n",
    "            self.weights1 = []  # Used in step-size update\n",
    "            self.weights2 = []  # Used in step-size update\n",
    "            self.weights3 = []  # Used in step-size update\n",
    "        \n",
    "    #### Needed for the decorator\n",
    "    def __setstate__(self, state):\n",
    "        super(SRPROP, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault(\"differentiable\", False)\n",
    "    \n",
    "    @_use_grad_for_differentiable ### This line is the decorator\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Args:\n",
    "            closure (Callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "                \n",
    "        params_with_grad = []\n",
    "        grad_list = []\n",
    "\n",
    "        #### PyTorch has in-built parameter groups which allow you to change hyperparameters for different layers\n",
    "        #### In this case all parameters in same group\n",
    "        \n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:       #### Iterating through layers\n",
    "                if p.grad is None:          #### .grad calculates gradient\n",
    "                    continue\n",
    "                params_with_grad.append(p)\n",
    "                grad = p.grad\n",
    "\n",
    "                if p.grad.is_sparse:\n",
    "                    raise RuntimeError(\"Rprop does not support sparse gradients\")\n",
    "                    \n",
    "                grad_list.append(grad)\n",
    "                state = self.state[p]\n",
    "                \n",
    "                if len(state)==0:       # First time optimizerz is called initialize internal state and track steps\n",
    "                    state[\"step\"] = 0\n",
    "                    state[\"step_size\"]  = (grad.new().resize_as_(grad).fill_(group[\"lr\"]))\n",
    "                    self.step_sizes.append(state[\"step_size\"])\n",
    "\n",
    "                state[\"step\"] += 1\n",
    "            \n",
    "            L, M = group[\"L\"], group[\"M\"]  # Use L and M hyperparameters\n",
    "            \n",
    "            if self.data_tally % L == 0 and self.lr_counter == 0:  # First iteration of lr-update on first call\n",
    "                self.weights1 = Clone_Parameters(group[\"params\"])   # Save network parameters\n",
    "                self.lr_counter += 1\n",
    "            \n",
    "            weight_update(params_with_grad, grad_list, self.step_sizes)  # Update weights, everytime\n",
    "            self.data_tally += M   # Add weight mini-batch size to data seen, everytime\n",
    "            \n",
    "            if self.data_tally % L == 0 and self.lr_counter == 1:  # Second iteration of lr-update\n",
    "                self.weights2 = Clone_Parameters(group[\"params\"])   # Save network parameters\n",
    "                self.lr_counter += 1\n",
    "                \n",
    "            elif self.data_tally % L == 0 and self.lr_counter >= 2: # Third iteration of lr-update\n",
    "                etaminus, etaplus = group[\"etas\"]\n",
    "                step_size_min, step_size_max = group[\"step_sizes\"]\n",
    "                self.weights3 = Clone_Parameters(group[\"params\"])   # Save network parameters\n",
    "                lr_update(group[\"params\"], self.weights1, self.weights2, self.weights3, self.step_sizes, \\\n",
    "                                 step_size_min, step_size_max, etaminus, etaplus)\n",
    "                self.weights3 = Clone_Parameters(group[\"params\"])\n",
    "                for i in range(len(self.weights1)):\n",
    "                    self.weights1[i] = self.weights2[i].detach().clone()\n",
    "                    self.weights2[i] = self.weights3[i].detach().clone()\n",
    "                self.lr_counter += 1\n",
    "                    \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efae29bf-bb61-46cf-bd1a-6c7886a2ac65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_update(params: List[Tensor],\n",
    "        grad_list: List[Tensor],\n",
    "        step_sizes: List[Tensor]):\n",
    "        for i, param in enumerate(params):\n",
    "            step_size = step_sizes[i]\n",
    "            grad = grad_list[i]\n",
    "\n",
    "            param.addcmul_(grad.sign(), step_size, value=-1)  # Update weights using individual learning rates and sign of gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "872bbff3-5e4e-490c-bc53-52369ac67905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_update(params: List[Tensor],\n",
    "    weights1: Tensor,\n",
    "    weights2: Tensor,\n",
    "    weights3: Tensor,\n",
    "    step_sizes: List[Tensor],\n",
    "    step_size_min: float,\n",
    "    step_size_max: float,\n",
    "    etaminus: float,\n",
    "    etaplus: float,\n",
    "    differentiable: bool=False,):\n",
    "    \n",
    "    for i, (param, w1, w2, w3) in enumerate(zip(params, weights1, weights2, weights3)):\n",
    "        if param.grad is None:\n",
    "            continue\n",
    "        step_size = step_sizes[i]\n",
    "        dw_epochA = w2.data-w1.data  #  Calculate first differene in weights\n",
    "        dw_epochB = w3.data-w2.data   # Calculate second difference in weights\n",
    "        if differentiable:\n",
    "            signs = dw_epochA.mul(dw_epochB.clone()).sign()\n",
    "        else:\n",
    "            signs = dw_epochA.mul(dw_epochB).sign()\n",
    "        \n",
    "        signs[signs.gt(0)] = etaplus     \n",
    "        signs[signs.lt(0)] = etaminus\n",
    "        signs[signs.eq(0)] = 1               \n",
    "        step_size.mul_(signs).clamp_(step_size_min, step_size_max)  \n",
    "    \n",
    "        ### for dir<0, dfdx=0\n",
    "        ### for dir>=0 dfdx=dfdx\n",
    "        restore = torch.zeros_like(signs, requires_grad=False)\n",
    "        restore[signs.eq(etaminus)] = 1                           # tracks which weights had gradient set to zero --> 1 otherwise 0\n",
    "        param.data.addcmul_(dw_epochB.detach(), restore, value=-1)    # reverts tracked weights back to w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c755059-0525-4fce-8095-138cd8637264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clone_Parameters(model_parameters):\n",
    "    Param_list = []\n",
    "    for p in model_parameters:\n",
    "        Param_list.append(p.detach().clone())\n",
    "    return Param_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b852b31-d18c-432b-9280-06de1fa886b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
